{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to do:\n",
    "- DONE: Paginate\n",
    "- DONE: Figure out what's up with Mutual CSV formatting issues (avoided the issue by cutting off at 1,000 characters)\n",
    "- DONE: Remove extra lines in the CSV\n",
    "- DONE: Search only for collections (aka, \"resources\") and items within collections (aka, \"archival records\")\n",
    "- Check to see if assumptions about notes aren't correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import some stuff you'll use\n",
    "import requests\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change these: Add your credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "USER = '#'\n",
    "PASS = '#'\n",
    "HOST = '#' #don't end with a slash\n",
    "def aspace_auth(host, username, password):\n",
    "    auth = requests.post(HOST + '/users/' + username + '/login',\n",
    "                        params={'password' : password})\n",
    "    if auth.status_code == 200:\n",
    "        token = auth.json()['session']\n",
    "        headers = {'X-ArchivesSpace-Session': token}\n",
    "        return(headers)\n",
    "    else:\n",
    "        return(False)\n",
    "\n",
    "headers = aspace_auth(HOST, USER, PASS)\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for any aspace records of any type with a given query term in any notes field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CHANGE YOUR SEARCH TERM AND PAGE SIZES HERE!\n",
    "q = 'holdings'\n",
    "LAST_PAGE = 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set things up so the filename is based on your search term\n",
    "filename = \"search_for_\" + q + \".csv\"\n",
    "\n",
    "# Create and open the file \n",
    "f=csv.writer(open(filename, 'w', newline=''),delimiter=',')\n",
    "\n",
    "#Create the first row of column headers\n",
    "f.writerow(['uri','title','identifier','external_id','record_type','note_with_search_term','note_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LET'S TEST!\n",
    "\n",
    "#Reset page to \"1\"\n",
    "page = 1\n",
    "page_num = str(page)\n",
    "\n",
    "#While page is less than whatever you entered above as the value for \"LAST_PAGE\"\n",
    "while page <= LAST_PAGE:\n",
    "\n",
    "    # Here is the actual search we're sending to ArchivesSpace!\n",
    "    # First we set up the crazy complicated URL for an advanced query of the \"notes\" field.\n",
    "    endpoint = '/repositories/2/search?page=' + page_num + '&type[]=archival_object&type[]=resource&aq={\"query\":{\"field\":\"notes\",\"value\":\"' + q + '\", \"jsonmodel_type\":\"field_query\",\"negated\":false,\"literal\":true}}'\n",
    "    # Then we send that search up to ArchivesSpace\n",
    "    results_full = requests.get(HOST + endpoint, headers=headers)\n",
    "    # Make sure our python script knows to read the JSON results as python data structures.\n",
    "    results_full=results_full.json()\n",
    "    \n",
    "    #Chris, Dolsy, and Rachel stopped here. \n",
    "    \n",
    "    # The results include a bunch of facet & page information at the top, which we want to skip.\n",
    "    # Below all the facet and page info, are the actual results. \n",
    "    # We'll isolate the part of the results that are really the results, excluding the facets & page info. \n",
    "    results=results_full['results']\n",
    "\n",
    "    # Add all the data!\n",
    "    # For each record returned by the search above . . . \n",
    "    for result in results:\n",
    "        # Define all the fields that you'd like put into the CSV.\n",
    "        uri = result.get('uri')\n",
    "        title = result.get('title')\n",
    "        identifier = result.get('identifier')\n",
    "        external_id = result.get('external_id')\n",
    "        record_type = result.get('types')\n",
    "        aspace_json = json.loads(result.get('json'))\n",
    "        # Below, we are making the assumption that each record will have the search term in only one note, \n",
    "        # and each multi-part note actually only has one subnote \n",
    "        # This returns the content of the first note that has the search term.\n",
    "        # Note for the user of this data: If you don't see the search term as expected, \n",
    "        # it may be in the record but not in the spreadsheet, so check ASpace. \n",
    "        notes = aspace_json['notes']\n",
    "        \n",
    "        for note in notes:\n",
    "            try:\n",
    "                subnotes = note.get('subnotes')\n",
    "                content = subnotes[0]['content']\n",
    "                #cut off the note content field if it's more than 1,000 characters, and add an elipse at the end\n",
    "                content = (content[:75] + '. . . [This note was cut off. Please check ASpace]') if len(content) > 1000 else content\n",
    "                note_type = note['type']\n",
    "            except:\n",
    "                content = note['content'][0]\n",
    "                #cut off the note content field if it's more than 1,000 characters, and add an elipse at the end\n",
    "                content = (content[:75] + '. . . [This note was cut off. Please check ASpace]') if len(content) > 1000 else content\n",
    "                note_type = note['type']\n",
    "            if q in content:\n",
    "                note_with_search_term = content\n",
    "        f.writerow([uri,title,identifier,external_id,record_type,note_with_search_term,note_type])\n",
    "    # finally, add 1 to the page so the next time the loop\n",
    "    # runs it will get the next page\n",
    "    page = page + 1\n",
    "    page_num = str(page)\n",
    "    \n",
    "# When everything is done, print below the page information\n",
    "last_page = results_full['last_page']\n",
    "last_page = str(last_page)\n",
    "print('Total pages: ' + last_page)\n",
    "print('Results are saved to: ' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASpace search queries that didn't work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query=\"q:Anything\"\n",
    "search = requests.get(HOST + '/search?', headers=headers, data=query)\n",
    "search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "endpoint = '/search?page=1&page_size=1&q=\"/repositories/2\"'\n",
    "\n",
    "results = requests.get(HOST + endpoint, headers=headers)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
